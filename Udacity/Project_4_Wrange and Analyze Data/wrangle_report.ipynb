{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e121d745",
   "metadata": {},
   "source": [
    "# Wrange report from WeRateDogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bc63e",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [Introduction](#introduction)\n",
    "- [First steps](#firststeps)\n",
    "- [Gather](#gather)\n",
    "- [Assess](#assess)\n",
    "- [Clean](#clean)\n",
    "- [Last thoughts](#lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e595f95",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## Introduction\n",
    "The following is a description of all efforts done to make the clean file `twitter-archive-enhanced.csv`. The code and steps done can be checked in `wrangle_act.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c96ea",
   "metadata": {},
   "source": [
    "<a id='firststeps'></a>\n",
    "## First steps\n",
    "The first thing to be done is to import all the necessary libraries to wrangle the data. In this particular case, the libraries used were pandas, numpy, requests (to retrieve information from internet) and os (to make sure the active directories were the correct ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf2530",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gather\n",
    "The data needed for the project was stored in different files:\n",
    "- The first file, `twitter-archive-enhanced.csv` was provided by Udacity within the project description. This would be the base of the project. The daframe was called `df`\n",
    "- The second one, `image-predictions.tsv` was provided by Udacity as well. However, it needed to be obtained programatically from Amazon Cloudfront. The requests library was necessary to retrieve the file and write it to the file `df_img`.\n",
    "- The third one, `tweet_json.txt` was not provedided by Udacity. It must be retrieved from Twitter directly. This was done by using their API via tweepy library. A function was defined to find all the information related to tweets stored in `twitter-archive-enhanced.csv` and store it in the txt file with a json format. After that, it was loaded as `df_json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37695a0",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assess\n",
    "The first step before cleaning the data is assesing it. A programatically exam through all the dataframes was done by using the functions \"head()\" and \"info()\". There were a lot of mistakes discovered:\n",
    "\n",
    "\n",
    "In the **`df`** dataframe:\n",
    "1. There were missing values for variables: \"expanded_urls\", \"in_reply_to_status\" and \"in_reply_to_user_id\" variables.\n",
    "\n",
    "2. Some of the values were not formated correctly: \"in_reply_to_status\" and \"in_reply_to_user_id\". They were formated as floats. They dhould be integers.\n",
    "\n",
    "3. Retweets data from \"retweeted_status_id\", \"retweeted_status_user_id and \"retweeted_status_timestamp\" were not useful for the project as it was not needed to analyze data about retweets. They had to be dropped.\n",
    "\n",
    "4. The data in \"doggo\", \"floofer\", \"pupper\" and \"puppo\" was not presented correctly. They were categorical values that should be stored within one column.\n",
    "    \n",
    "\n",
    "In the **`df_img`** dataframe:\n",
    "1. Data from variables \"p1\", \"p2\" and \"p3\" was not standarized. The variables were presented in upper and lower case.\n",
    "2. The character \"_\" was used to represent blank spaces.\n",
    "3. This dataframe should be part of the main `df` to satisfy tidiness requirements. They had to be merged.\n",
    "\n",
    "\n",
    "In the **`df_json`** dataframe:\n",
    "1. There were missing values in variables \"geo\", \"coordinates\", \"place\" and \"contributors\".\n",
    "2. There were also missin values in \"in_reply_to_status_id\", \"in_reply_to_user_id\" and \"quote_status_id\".\n",
    "3. Retweet information was not useful for the project as well. It had to be removed.\n",
    "4. Variable \"id\" did not match the main `df` dataframe. It should be renamed.\n",
    "5. Variables \"in_reply_to_status_id\", \"in_reply_to_user_id\" and \"quote_status_id\" were wrongly formated. They were floats and should be integers.\n",
    "6. \"id_str\", \"in_reply_to_status_id_str\", \"in_reply_to_user_id_str\" and \"quoted_status_id_str\" were redundant an not needed for the project. They had to be removed.\n",
    "7. Variable \"posibly_sensitive\" was formated as a float. It should be a boolean.\n",
    "8. Variable \"posibly_sensitive_appealable\" is a legacy value. It had to be dropped.\n",
    "9. This dataframe should be part of the main `df` to satisfy tidiness requirements. They had to be merged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2dc31",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## Clean\n",
    "The steps and code done for cleaning the dataframe can be consulted in `wrangle_act.ipynb` notebook. It is better explained there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53ed5b",
   "metadata": {},
   "source": [
    "<a id='lt'></a>\n",
    "## Last thoughts\n",
    "The final result is `twitter_archive_master.csv`. However, a few important facts should be noted.\n",
    "\n",
    "When retrieving all the tweets data from Twitter API, not all of them were found. This is due to those tweets been deleted. This has a negative impact in the data completeness. Only half of them are as detailed as needed to be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
